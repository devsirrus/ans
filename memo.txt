abc123
CT202600
https://eden.ac/login/ctct

PUBLUS LITE(電子テキスト:製本受領済み)
https://ctct.publuslite.com/
PL200923
VLKRFDJAJQKW


7.0でVCへのweb client(Flash base)が廃止
・ESX: esxcli
・vCenter: powercli

ESXi 6.5 later
UEFIセキュアブート
コアダンプの暗号化
が可能になった。VCP試験的に機能の内容を確認すること。

ディスクレスホストでESXiで起動する(PXE)Auto Deploy機能の概要を確認。
ESXiのインストールも自動化可能。

・DCUIの設定
DNS/IP/DGW以外に
キーボードレイアウト、トラブルシューティングサービスの有効化(ESXi Shell/SSH) ※デフォルト無効
GUIから後で設定も可能。
仮想マシン状態のパフォーマンス確認はコマンドの方が詳細に見れる。

・ESXのユーザー管理
デフォルトrootのみなのでrootは厳密に制御
それ以外のユーザ管理が必要かといえばVCenterがあるので通常は不要。

細かいユーザ管理をESX単位でする場合は大変なので、ADユーザを使い権限制御するのがよい
ADの定義はユーザの定義のみで、権限定義はvSphere(vCenter/ESX)側でやる
例外：ESXAdmins ADグループに含まれるユーザはesxiが自動的に管理者権限を付与する。vCenterではこの手段は使えないので普通に権限設定が必要。

・ntpの設定
GUIの証明書などにも関連するのでntpクライアント設定が望ましい


・vmware tools
Linuxの場合は公式よりopen-vm-toolsの方がよい。
公式もそう言っているベストプラクティス

・appinfoプラグイン
vmware tools 11.0以降。仮想マシン内のアプリケーションの情報取得が可能。
# 仮想マシンverの制約もあるっぽい
単体利用ではなく、他サービス(AppDefence)との連携想定。NSXなどでは特定のアプリが～なら、通信を～などができる
vsphere7では仮想マシン単位で無効にできる。
まだコマンドのみでの設定する。（最新の機能はコマンド/api経由のみのは多々あるらしい)
 - 状態の確認
esxcli vm appinfo get
 - プラグインの状態設定
esxcli vm appinfo set --enabled {True | False}


◆仮想マシンファイル ★必須
★VM_name.vmx	構成ファイル(HW設定など) ただのテキストファイル 他はバイナリ
★VM_name.nvram	BIOSファイル OSインストール後は変更不可(BIOS-> UEFIなど)

★VM_name.vmdk	ディスク ディスクリプターファイル 
★VM_name-flat.vmdk	ディスク データファイル。実データ(サイズが大きくなる)。
⇒2つで仮想ディスクを構成する。

VM_name-flat.vmtx	テンプレート構成ファイル
VM-name-*.vmss	サスペンド状態ファイル
.vswp	競合時にメモリを再要求するために利用するスワップファイル
.log	仮想マシンの最新ログファイル
#.log	古いログのアーカイブに利用するファイルセット

・仮想ハードウェア
MAX値は仮想ハードウェアのver(仮想マシンの互換性)でかわる。後方互換性(古いESXiに搭載)の要件がなければ最新にするのがよい。
ex)
互換性/仮想ハードウェアver
ESXi 7.0 / 17
ESXi 6.7U2以降 / 15
など

・仮想disk設定
サイズ
⇒割り当てHDD容量

SCSIコントローラー
⇒ストレージコントローラ。ハードウェアコントローラを仮想マシン用にエミュレートしたもの
   インストールするOSに対してデフォルト値が決まる。(大抵はLSI Logic SAS)
   大体デフォルトで問題ない。
   VMware Pravirtual SCSI
   使いたい場合はVMware toolインストール後にタイプを変更する。
   SASよりディスク性能が向上する。
   が、やるかは微妙らしい。SCSIプロトコルではなく、独自プロトコル(特殊SCSIコマンド)を使う場合(クラスターなど)
   は不具合の原因になるとのこと。

   Virtual NVMe
   仮想マシンver 13～が必要。

プロビジョニングタイプ
⇒シック(Lazy Zero レジー・ゼロ) VMFS利用のデフォルト値
⇒シック(Eager Zero イーガー・ゼロ) 
共通：作った段階で割り当て容量を消費する。ゲストOS内のディスク使用量は関係ない。
       ⇒割り当てHDD容量 = 仮想HDDファイルサイズ

Eager-Zero:予めゼロ埋めする。(のでゲストHDD未使用領域は0)
Lazy-Zero: ブロックへのデータ書き込み時にゼロを書き込む(ゲストHDD未使用領域はnull)

⇒シン NASに置いた場合のデフォルト値
仮想HDDサイズは可変長。
仮想HDD割り当て容量と関係なく、ゲストOSのディスク使用量に応じた仮想HDDサイズとなる。
 ⇒ストレージのオーバーコミットが可能となるがストレージの管理が困難のため通常利用しない。

3つのどれが一番いいかはベストプラクティスはない。ケースバイケース。
パフォーマンスの観点では以下順で早い。VM作成時間は逆順で早い
シック Eager-Zero ⇒ シック Lazy-Zero ⇒ シンプロビジョニング

Eager-Zeroは実際に0を書き込むのでHDD初期化(作成)に時間がかかる。イメージとしてはWindowsのフルフォーマット
Lazy-Zeroはnull埋めなのでHDDの初期化は瞬時。イメージとしてはWindowsのクイックフォーマット。

モード(あまり触らない項目 snapshotに関連)
(1回目はスキップ。後日)


・仮想ネットワーク
・NIC
完全仮想化NIC
・e1000/e1000e 
デフォルトで使われているNIC。完全仮想化のため仮想マシン内で物理NICをエミュレート
最も無難。仮想専用ではないので準仮想化にパフォーマンスは譲る。

準仮想化NIC
・VMXNET3
要VMware tool 仮想マシン専用ドライバ。
NWパフォーマンスが必要であればtoolインストール後にNICタイプを変更する。

パススルー(例外的)
仮想SW経由でなく、ESXホストの物理NICを直で認識させる。
通常のNICではなく、HBAで使うことが多い。(vSphereは仮想HBAが使えないので、VM内でHBAを認識させるのに利用)

直結したらほか仮想マシンがそのNIC/HBAが使えなくなるか？
⇒Direct Path I/Oでは使えなくなる。
  SR-IOV/PVRDMAでは複数の仮想マシンで共有可能。

・Direct Path I/O
⇒vmwareの独自機能。昔(vsphere4)からあるが色々不便だったらしい。(上記占有など)
  # 最近は動的Direct Path I/Oというのが登場して検討の余地ができたらしい。
  
・SR-IOV
Direct Path I/OをもとにIETFが標準化。パススルー使う場合はこれを使う。

・PVRDMA(6.5～)
RDMAという低遅延ネットワーク用インターフェースをパススルーするための専用規格。
通常はイーサネットのため使うことはない。



◆vCenter
ESXi/ESXi上の仮想マシンを管理するサーバ。直接仮想マシンを実行しない。
昔はWindows用があったが、vSphere7からはLinuxベースアプライアンスのVCSAのみになった。
# 超面倒なDBのセットアップなどが不要となった。

VCSAの中身
・VMware Photon(Linux OS)
・PostgreSQL
・vCenter Serverのサービス
  └ vCenter Server
  └ vSphere Client
  └ vCenter Single Sign-On
  └ ライセンス サービス
  └ vCenter Lookup Service
  └ VMware CA
  └ コンテンツライブラリ
  └ vSphere ESXi Dump Collector

最近はvCenterは他のVMwareプロダクトとの接続点の役割が出てきた。(vSANの管理など)

・vCenter Single Sign-On
他のVMwareサービスにもSSOを実現する。vCenter 2台の管理に対しても有効。(拡張リンクモード)
ケルベロス認証(AD)とSAMLトークンを利用する。
# vCenter1台だけのみの利用だとSSO感がないのでピンときづらい

vCenter2台～を拡張リンクモードで同一ドメインにしたい場合は
スケールアウトなどのシナリオが考えられる。検証/本番のvCenterを同一ドメインで統合することは通常ない。(本番と検証が同じ画面で管理されることになるため)
なお、今現在拡張リンクモードはvCenter Serverの新規作成時にしか有効化できない。
後からスケールアウトのために有効化するのは不可なので予め拡張リンクモードのグループ構成が必要。


・ESXi ～ vCenter間の通信
443で通信。

・ロックダウンモード
セキュリティ向上機能
ESXの直接ログイン(VMware Host ClientでのESXログインやssh)を不可として
vCenterからのログインのみにできる。

実運用上は
・vCenterが落ちた時に困る
・監視など直接ログインのニーズは多々ある
のであまり使われない。


----
★Notes vCenter紛らわしいクライアント
vSphere Client
-> vCenter Serverインベントリの管理

vCenter Server 管理インターフェース
URLが:5480ポートでログイン
見た目がとても似ているが別物。VCSAのLinux部分の管理。(VCSAのバックアップ取得/ホスト名変更/IP変更など)
★SSOでログインしない。Linuxのユーザでログインする。
# 慣れていれば通常通りsshでファイルを編集して設定変更してよいらしい。

◆ライセンス
ESXライセンス
vSphereライセンス

vCenterを使う場合はvCenterのライセンス サービスでESXのも管理する。(vCenterのライセンス管理画面にESXのライセンスキーを入れる形)
ESX単体利用の場合はESXで入れる。
# ライセンスサービスはデフォルトで起動している。

ライセンスコードは個別に生成可能なのでvCenterがインターネットにつながっている必要はないが
パッチの適用などが楽にできるので、Proxy経由などでインターネットにつながることが望ましい。

ESXiライセンス 購入の単位
・CPU単位

有償無償
・無償
60日評価期間ライセンス
⇒エンタープライズプラスと同等機能

vSphere Hypervisor(永続)
⇒My VMwareのサイトから誰でも手に入れられる
⇒機能制約がとても強い。vCenterが利用不可のためこれならKVMと大差ない。

・有償
エッセンシャル
エッセンシャルプラス
⇒エッセンシャル系はとても安いが機能制限強い(小規模環境向け)

スタンダード
エンタープライズプラス
⇒通常はこっち

・vCenterライセンス 購入の単位
インスタンス単位

有償無償
・無償
60日評価期間ライセンス

・有償
Foundation
Standard


# MyVMwareからDLできるソフトウェアはパートナーでないとDLできないものがある(NSXなど)

◆ナビゲーターのアイコン(4つ)
・ナビゲーター
vCenter画面の左端のやつ

左から1番目
- Host and Clusters
Hostと(Host配下)VMの一覧。

・左から2番目
- VMs and Templates
Host and Clusters同様仮想マシン表示される
Hostが表示されなくなる。
⇒VMの一覧表示(ホストは考慮しない)

・左から3番目
- Storage
データストア情報

・左から4番目
- Network
標準SW/分散仮想SW


◆データセンターオブジェクトについて
vCenter上のESXi登録先。
管理の単元なので、物理的DCと一致させる必要はないが、同様の理由で一致させることが多いらしい。
DCオブジェクト内を階層管理するために、フォルダ、サブフォルダを作成することが可能。
フォルダを作るとアクセス権の設定が楽になる。(VM操作権限)

データセンターオブジェクトを分けることで発生する制約
⇒DCを超えて移行系(vMotionなど)やクラスタの構成はできない。

◆カスタム タグの作成
インベントリのオブジェクト(VM)にメタデータを付けられる。
タグを利用した検索やソートができる。
なお、検索はタグだけでなくワイルドカード、仮想マシン状態、VMware Tool実行状態の検索など色々できる。

タグは複数つける事が可能。

◆ADの参加
参加自体は簡単だが、vSphere Client or 管理コンソールからvCenterの再起動が必要。
再起動は時間がかかる(10min～)
OSの再起動はすぐだが、vCenterのサービスが重い。
ADのドメインとvCenterのSSOドメインは異なってよい。

◆vCenterの権限
権限ベース制御
ロール：
権限の集合。デフォルトロールが多数ある。
大まかに２種類
・システムロール
管理者/読み取り専用/アクセスなし/暗号化なし管理者
⇒規定値からの変更はできない。例えばROユーザの権限に一部変更権限を付与することはできない。
⇒暗号なし～(6.5から仮想マシンの暗号化など暗号化系の操作ができた。暗号化以外の操作が可能なユーザ)

・サンプルロール
変更が可能。ただ、サンプルロールの意義はユーザ定義ロールのテンプレートのため
サンプルロールをコピーしてユーザ定義ロールを作成する形で使う。


作成したロールはオブジェクトに適用する。
1. オブジェクト選択
2. ドメインからuser or groupを選択
3. ロール選択
4. 子オブジェクトに権限を伝搬(Propagete)

・伝搬
⇒フォルダに権限を与え、子オブジェクト内に伝搬させるとフォルダ内のVMにも権限が適用される。

⇒親オブジェクト 例えばDCにROを与えて、子オブジェクト(フォルダや個別のVMなど)でAdminを与えた場合
  より深い階層の権限が適用される。no access権限はこの様に利用する。(親階層の権限の否定)

⇒複数の権限グループ付与
  競合しない権限の場合：共存する(ex snapshot権限とpoweron権限)
  競合する権限(ex 管理者/アクセス不可)の場合： 一番先に設定した設定が適用される。

⇒グループに対する設定とユーザへの設定の併存
  ユーザに対する設定が優先される。PowerOnグループとアクセス不可ユーザの場合アクセス不可。

・vCenter全体に権限を持つユーザ

user_Aを作成し、管理者ロールを付与、vCenter(最上位)に適用すれば
vCenter全体に有効な管理者ユーザが作成されるように見えるが、拡張リンクモードで複数のvCenterを管理する場合は、ほかのvCenterには権限を持たない。
SSOに対するオブジェクト(グローバルオブジェクト)があるので、それに適用すればvCenter全体に管理者権限が与えられる。
ただし、グローバルオブジェクトは個別のvCenterの持ち物ではないのでvCenterでは見えない。
グローバルオブジェクトの権限は「アクセスコントロール」-「グローバル権限」で設定する
なお、コンテンツライブラリやタグはグローバルオブジェクト直下のオブジェクト。(vCenterオブジェクトの配下ではない)

・その他
デフォルト値でオブジェクトに付与されているロールは変更してはならない。(Administraotrなど)
ログイン後の権限は次回ログインまで維持される(ログイン中のユーザの権限を変えた場合、反映されるのは次回セッションから)



◆vCenter バックアップ/リストア
6.5からの機能
vCenter管理I/Fを利用。
FTPS/HTTP/HTTPS/SFTP/FTP/NFS/SMBを利用してネットワークドライブへのバックアップを利用可能。
バックアップ暗号化可能。

※vCenterのバックアップであり、ESXiや仮想マシンのバックアップではない。

・ファイルベース バックアップリストア
管理I/Fの標準機能。設定のバックアップリストアが可能だがLinux システム破損の復旧には無力。
ファイルベースのバックアップリストアを即時実行 or スケジュールできる。
スケジュールの場合は保管ポリシーの適用/バックアップ失敗時のアラームのトリガーが可能。

VCSAのGUIインストーラからリストアする。(vCenterのメニューではない。そのためvCenterの破損の程度はリストアの実行可否に関係ない)
リストアを実行すると新しいVCSAがデプロイが行われ、そこに設定復旧が行われる。

アップデートなどでvCenterのビルドが変わったらそのインストーラを用意しておく必要がある

・イメージベース バックアップリストア
vSphere Storage APIs - Data Protectionと連携する市販のソフトを利用して行うなど。


◆VCSAの監視
・イベント
デフォルト30日保管。30日単位で選択。
vCenterのディスク使用はイベントとパフォーマンスデータが大体を占める。

・ログレベル
VCenter/ESXi/仮想マシン用のログはそれぞれ別。
vCenterのログレベルのデフォルトはInfo
ログレベルの変更は容易(即反映/再起動不要)なので複雑なトラブルシューティングを必要な時だけVerboseやTriviaにする考え方もある。
ログはsyslogに転送できるので、syslog管理が一般的。
# vRealize Log Insightなどの有償製品と組み合わせてログ解析することも可能。

・DBの健全性
DBは非常に重要。DBが実行できないとvCenterは使えない。
DBのステータスはデフォルト15分間隔でチェックされる。
DBの使用容量が80%で警告/95%でエラーかつ、vCenterサービスはシャットダウンされる。(継続したパフォーマンスデータ収集でデータを溢れさせないため)
VCSAは仮想ディスクを13本持つが、仮想HDD 5～7がDB用でここがモニタリングされている。
DBパフォーマンスが必要なら、仮想HDD5～7を高速ディスクに置き、vCenterの性能改善をする手法がある。

・サービスの稼働状況/停止起動
管理コンソールから確認、実行が可能

・パッチ アップデート
VMwareは毎月VCSAのセキュリティパッチを提供している。
・重大な脆弱性に関するパッチ：月一のリリースサイクル
・高レベル/低レベル脆弱性 次回vCenterのパッチ/updateで提供

インターネットにつながっていると利用可能なパッチの自動チェックを定期実行できる。

◆vCenter Server High Availability
vCenterの冗長化。vCenter HAとvSphere HAは別物。
vCenterは管理しているだけなので、ちょっと前までvCenterの冗長化は重要でなかった。
現在は、他VMware製品との連携ポイントの役割があるため、冗長化の必要性が出た。
ex) SRM/NSX/Horizen/..etc

・esx6.0以上かつ、VCSA6.5以上
・vCenterの規模「small」以上
・共有ストレージは不要。
・ライセンスは1つのvCenter Server Standardでよい

クラスタ構成
・アクティブノード：現用系。パッシブとDB/ファイルのレプリケーションを行っている。
・パッシブノード：待機系。
・ウィットネスノード：スプリットブレイン保護のためクォーラムを提供するノード。スプリットブレイン監視用の軽量ノードであり、vCenterの機能は持たない。
                      ウィットネスノードが落ちているとスプリットブレインなのかActiveのダウンか判定できないため、フェイルオーバーができない。

vCenter HAはウォームスタンバイ(サービスは停止している)ので切り替え時間がかかる。
# 猶更vsphere HAでいいような気がするのだが。。。


◆仮想スイッチ
ESXi内の論理L2SWで、仮想マシンのアクセススイッチとなる。2種類ある

・標準スイッチ(vSS:vSphere Standard Switch)
各ESXi単位で作成/設定する。
制御プレーン,I/OプレーンはESXiで実行される。

制御プレーン：設定(VLANやポートグループなど)
I/Oプレーン: パケットの送受信を行う。

・分散スイッチ(vDS:vSphere Distributed Switch)
vCenter管理。ESXi全体に跨る1つの仮想スイッチ。
何台ESXiがあっても1つのvDSの設定で済むほかvDSでしかできない高度な機能がたくさんある。

制御プレーンはvCenter実行し、I/OプレーンをESXiで実行
⇒設定管理はvCenter/実際の通信はESXiで行う。よってvCenterが障害を起こしていても通信に影響はない。

ただし、VCSAリストア時のデプロイ時に選択できるのはvSSのみなので(ESXiにデプロイするため)
管理用NW(DNS利用可)はvSSにしたほうがよい。全部vDSにするとリストア時にNWを選べなくなる。

要EnterPrise Plusライセンス or vSphereスタンダード + vSAN(or NSX/Horizon/view)
1vDSに対し、最大2000台接続可能


・仮想SW接続の種類
・仮想マシン ポートグループ
仮想マシンが接続されている仮想SWのポート。VM NetworkXXと命名されているもの。
1ポートでもポートグループだが、デフォルトでは1ポートグループを作ると8ポートできる。
ポートグループに対して設定を行う。
なお、HA DRSを利用する場合複数ESXiで同じポートグループ名である必要がある(大文字小文字含む)

・アップリンクポート
仮想SWを物理NICに繋ぐためのポート。そのため通常2つ(2NIC)以上ある。

・VMkernelポート
ESXi管理ネットワーク関連のポート。ESXiのIP/共有IPストレージ(iSCSIやNFS)/vMotion/FT/vSAN/Replication利用など。


仮想SWは複数作ることができる。
用途を仮想SW単位で分けるか、1つの仮想SWでVLAN単位で分けるかの使い分け。
仮想SWを分けるとアップリンクを分けられるが、その分NICやNICの接続先L2SWポート/配線が必要になる。
仮想SW単位が推奨されてきたが、上記理由で現実的には1つの仮想SWにてVLANで分ける事が大半。

アップリンクのトランク設定は明示的には無く、VLAN IDを設定するとトランクになる。
IPを持つのはNICではなく、各仮想マシンやVMkernelポート


◆vSSのポリシー
標準スイッチレベル：vSS全体に適用
ポートグループレベル：ポートグループに適用。標準スイッチレベルより優先される。

・セキュリティ ポリシー
# 補足資料P25
# 名前と裏腹にセキュリティ設定ではない。どちらかといえばL2SWの挙動設定。
Promiscuos
自分MAC宛以外のフレームも受信。AcceptはL2SWのフラッディング状態。

MAC address changes
ESXiが割り当てたMAC以外の宛先MACの受信許可。
クラスタのvMACのように、1NIC複数MACの話。

Forged transmits(偽装転送)
ESXiが割り当てたMAC以外の送信元MACで送信を許可
MAC変更の逆方向版なので、ペアで設定するのが通常。


作り方でデフォルト値が異なる。
vCenterで作成：Reject/Accept/Accept
ESX(host client)で作成：上記3つがReject状態 ※vsphere7からこう。6はReject/Accept/Rejectらしい
。万一ESXiでvSS作成する場合は注意。


・トラフィックシェーピング
帯域制限。LANの中で設定する機会は少ない。

・NICチーミング/Failover
# デフォルト設定が最も良いとされるらしい。
Act/ActもAct/Sbyも可能。Act/Actの場合ロードバランスアルゴリズムも考慮する。

(一応)関連パラメータ
Load balancing
- 仮想ポートID(デフォルト)
仮想ポートIDに基づき物理NICにマップ
メリット)
物理SWの設定に依存しない。
デメリット)
負荷が均等になるとは限らない。
NIC障害時に数秒掛けるのでユーザ影響あるパケロスは絶対起こる。(これが痛い)

- 送信元MACハッシュ
仮想NICのMAC別にマッピング。そのため、仮想ポートIDとほぼ同じ結果となるが
複数MACがある場合はそれぞれにマックされる。
それ以外のメリデメは同様。

- 送信元及び送信先のIPハッシュ(リングアグリゲーション)
切り替え時間が高速なのが最大のメリット(理論値は10マイクロsec以下)
重要なアプリを保護するのに最も適する。

アップリンクのSWとLAGの設定を組む。


ビーコン：チーミングの各NICから間接リンク障害検出用のbroadcastを定期的に送る。
           間接リンク障害がなければNIC1が送ったビーコンはNIC2,NIC3で受信できる。
           仕組み的にNICが3枚以上必要だったり、各NICが同一ブロードキャストドメインにいる必要がある。
           # 使い道が難しいのでSW側でリンクステートトラッキングにて間接リンク障害対策する方が妥当。


Notify switchs：vMotionでVM搭載ESXiホストが変更された場合、接続L2SWのMACアドレステーブルを更新する必要がある為
                仮想マシンのMACにてG-ARP通知する行為をこの様に呼称している。



◆仮想ストレージの設定と管理
・データストア
仮想マシンの置き場所。

・データストアの種類
・VMFS(VMware File System)
ローカルディスク/FC/FCoE/iSCSI

VMFS5と6(ESX6.5～)がある。
VMFS6は1セクタが4k(4096byte)になった
# ESXは4kセクタ対応が遅かった背景がある(512eで凌いでいたらしい)


仮想マシンのデータ格納用に作成されたFS
共有ストレージ(LUN)への同時アクセスが可能。(クラスタファイルシステム)
⇒複数のESXiが同一のLUNを利用できる。

動的拡張

オンディスクロック
 データブロックの単位でロックができる

・NFS
・vSAN

・vSphere Virtual Voulumes
API経由でvSphereがストレージ装置の機能を利用できるFS

ストレージ装置がvASA(バーサ)とVVOLに対応している場合は
vSphereがAPI経由でストレージ装置側の機能を仮想マシン単位で利用できる。
本来LUN単位で実行するストレージ装置の機能を仮想マシン単位で実行できたりする

・RDM(Raw Deivce Mapping) ※これは仮想マシンの設定
VMに直接LUNを割り当てる機能。(VMのディスクは.vmdkファイルでなくLUNそのものになる)
利用シナリオ
 仮想Diskの制約(ex ～62TB)などを受けない。 # 昔は最大値2TBとか少なかった
 VMと物理サーバ間 クラスタ構成の共有ディスクを構成する

# vsphereがサポートするネットワークディスクのプロトコル
iSCSI
FC
FCoE
(FCIP/iFCPはNG)
NFS(CIFSはNG)

◆FC
・FCID(24bit)
IPに近いもの
FCはアドレスの手動設定の概念はない。初期化(FLOGI:ファブリック・ログイン)中にFCSWから割り当てられる。

・WWN
WWN: FCノードに割り当てられる識別子(64bit)であり通信には使わない。ストレージI/Oのマスキングに利用

・ゾーン
FCスイッチのポート単位でVLANのようなもので組む
ポートでzone作る(hardware zone)ことも
WWNでzone作る(software zone)こともできる

・LUNの識別(検出)
何もせずともOS(ESXi)から勝手に見える
ESXiの起動時のシーケンスでスキャンされるので、ホスト起動後にストレージの設定変更(LUN追加など)をしても
自動的に見えない。手動で再スキャンする必要がある。

・マルチパス
ESXiにマルチパスドライバーは同梱されている。
ロードバランスと冗長化を実現

◆FCoE
ホストにCNA(統合ネットワークアダプタ)が必要
OSからはNICとHBAがあるように見える(ネットワークドライバ/FCドライバがそれぞれある)
アップリンクのFCoEスイッチがLANとFC SANへの分配を行う。

サーバがFCとイーサネット両方に直結する必要がなくなる。
速度/コスト的メリットはない。完全新規導入時に選択肢に上がる程度なのであまり採用されていない。

◆iSCSI
TCP/IPでSCSI I/Oをやり取りする。(SCSIコマンドをローカルでなく、IPネットワーク越しでやり取りする)
IPが使えればL2プロトコルはなんでもよい(ethでなく802.11やPPPでも)
SCSIコマンドをTCP/IPでカプセル化する。[SCSI/iSCSI/TCP/IP/L2 Proto]

iSCSIイニシエータ。
 iSCSIが喋れるサーバのこと。ハードウェア/ソフトウェア実装両方ある
 HW：NICにオフロード
 SW：OSでソフトウェア実装。こちらが一般的

iSCSI ターゲット名
FCのWWNに当たるものでWWN同様LUNマスキングに利用する。こちらはHWでなくソフト的な設定値。
エンドーツーエンドで変わらないノードの識別子(IPやMACは変わる可能性がある)
iqn.～や(iSCSI Qualified Name形式)
eui.～で始まる(Extended Unique Identifier形式)

iSCSI イニシエータ名
...

iSCSIエイリアス # ランタイム名のこと???
...

ランタイム名
iSCSI ターゲット名+LUN番号は長いのでコマンドなどで利用するのが大変。
利用しやすくするためにエイリアス名を付けたもの。
命名規則vmhbaN:C:T:Lに従い再起動後維持されない。


ESXiホストがiSCSIを使う場合事前にSW or HW　iSCSIアダプタの設定が必要。
ホストはiSCSIイニシエータを使用してiSCSIターゲットにアクセスする

 独立型HW：ESXiから独立しており、HWに設定する。(iSCSIイニシエータ設定をESXiに行わない)
 依存型HW：設定値はESXiに依存する。


・ESXiはデフォルトでiSCSIのソフトウェアアダプタ無効なので有効化する
iSCIターゲットの検出を行う
 静的検出(Static) ストレージ+接続先LUNを1つずつ設定する。SANブートの場合はどこからbootするか指定が必要のためこちらが必要。
 動的検出(SendTargets) 原則はこちらを利用。SendTargets応答でIQN及び使用可能な全IPを返される
 # メールサーバを利用した検出は不可

iSCIセキュリティ設定としてCHAPが利用可能。デフォルト無効。ターゲットごとに可能。

・マルチパス
独立型HWアダプター：HWに設定
ソフトウェア or 依存型HWアダプター：ESXiに設定

同じサブネットに複数のVMkernelポートを作っている場合
どのVMkernelをiSCSI/物理NICに使うかの設定が必要(ポートバインド)

マルチパスする場合は複数のVMkernelポートとIPを設定しそれぞれ別NICへのポートバインドをすることでマルチパスする。(イニシエータiqnは共有される)
現在のESXiはAct/Sbyのみでロードバランス不可(iSCIの話。FCは可能)


◆VMFSデータストア
シンプロビジョニングしている場合は、
ディスク使用率残が少ないのをレポート機能でメールやSNMP通知する。(アラーム)
データストアディスクの過剰割り当てや仮想マシンのディスク使用率をアラーム設定する

・VMFSデータストアのサイズ拡張
エクステントの追加
 LUN1で構成しているデータストアをLUN1+LUN2のデータストアに変更
 LUNの最大値は32個。合計エクステントサイズの上限は64TB
 縮小はできない。

既存のエクステントのデータストアを拡張
 縮小はできない。

・データストアのメンテナンスモード
メンテナンスモードにする前に、すべての仮想マシン及びテンプレートを別のデータストアに移行する必要がある
通常はStorage DRS利用時に使う。(複数のデータストアを1つのデータストア クラスタにする機能)
データストアクラスタの一部のデータストアをメンテナンスモードにするなど。

・VMFSデータストアの削除/アンマウント
削除
 データストアを消せば中のデータ(仮想マシン)も消える

アンマウント
メンテナンス時、データストアの一時的にオフラインにする機能

・ストレージのロードバランシング
ストレージベンダーはマルチパスドライバーを提供できる。
# なお、Active/Activeアレイでなければ、ロードバランスしてはだめ

スケーラビリティ
 ラウンドロビン:1000IOPsごとに切り替え

可用性
 MRU(最近の使用)：自動切り戻しなし
 fixed(固定): 切り戻しあり

◆NFS
NFS3 or NFS4.1が利用可能。
4.1ではStorage DRS,Storage I/O Controlが使えない。あとSRMも。

ESXからはLUNでなくネットワークドライブとして認識される。
マルチパスはvSWのチーミングで行う。
複数のNICを効率よく利用する場合はLAG(送信元/送信先IPハッシュ)で行うのがよいが
NFSがIP2つもっていないとハッシュが複数にならない。
⇒マウント先のIPを2つ設定(セッショントラッキング:NFS4.1)

◆vSAN
vSphereとは別の製品。Software-Defined Storage
各ESXのローカルディスクを仮想的に結合して共有ストレージを提供するHCI機能。
HWやネットワークの要件が厳しいのでvSAN Ready Nodeを買うのが無難。

ファイルシステムはファイルベースやブロックベースでなく、
オブジェクトベース(ストレージ)となる。

仮想マシン単位でストレージポリシー(RAIDレベルやパフォーマンスなど)の設定が可能。


◆テンプレート
VMのテンプレート。
作成したテンプレは運用上メンテは必要。(ex WindowsのSP1->SP2など)
一度作成したテンプレは変更できないので、仮想マシンに変換、設定変更後にテンプレに再変換する。

・作り方(要VMware tools)
VMから作成
 作成元のマシンはpowerOn/Offを問わない(元のVMに影響ない)

VMから変換
 作成元のマシンはpowerOffである必要がある。(元のVMは消える)

既存のテンプレートから複製


◆クローン
VMから直接VMをコピーして作成する。
クローン元はpowerOn/Offどちらでもよい。
テンプレとの違いはテンプレファイル化を挟むかどうか

◆ゲストOSカスタマイズ
テンプレートやクローンから作成したVMの設定値を変更できる。IPやホスト名など。

・カスタマイズ仕様
全パラメータを変えるか、一部だけ変えるかを予め作成しておく。
# 全部を手入力せずに一部のみにする。

◆クローンの種別
・フルクローン
コピー元の完全コピペ。元が100Gならコピー先も100G

・インスタントクローン(vSphere6.7～)
# Horizonからの輸入
使い捨て、一時的利用用VM向け。vSphere Clientからは操作できない(API経由)
クローン元とsnapshortベースのディスク共有される(同じ部分を共通化)
仮想マシンの電源を切ると消える。

◆コンテンツライブラリ
vCenter同士でOVFなどデータを共有/同期する機能。
第三者を交えたテンプレの共有、更新などに。パスワード設定可能。

ライブラリの種類
ローカル
パブリッシュ：公開状態
サブスクライブ：パブリッシュの内容をDLして同期しているもの(メタデータ同期)


◆仮想マシン設定の変更
CPU/Memはパワーオン時の変更に一部制限有

・ホットプラグ
HDD/NIC/USBはホットプラグ可能
CPU/MemはHotPlugの設定を有効にすれば可能だが、この設定項目はpoweroffの時にしか変更できない。
実際にOSが認識するかはゲストOS次第

・仮想ディスクサイズ 動的拡張
  パワーオン時に可能。

・シンプロビジョニングディスクの拡張
  powerOffしてvmdkを拡張(inflate)する。実態はシン -> シックのEager Zeroedへの変換。逆は無理。
  実際は上記制限(パワーオフ&型方向の変換)のため、Storage VMotionの機能を利用することが大半。
  オンラインかつ変換方向の制限がない。



・仮想マシン表示名
  ここを変えても仮想マシンファイル名についている名前は変わらないのでなるべく変えないこと。
  Storage VMotionで表示名 = 仮想マシンファイル名に修正は可能。

・VMware Toolsの設定
  仮想マシンのパワーボタン動作のカスタム
  VMware Toolsのアップデート(こちらは個別のVMでやるべきではない。vSphere7からライフサイクルマネージャで統合管理可能)
  # 特にWindowsはVMware Toolsの更新 = 再起動

・仮想マシンの起動設定
EFI/BIOS
OSインストール後の変更は不可。
EFIを選択するとセキュアブートが可能になる。

仮想マシンの起動失敗後に再試行する
⇒起動失敗の要因にリソース不足があるので、10秒後のリトライで解消することもある
  (あまり使わない)

◆vMotion
VMのコールド/ホットマイグレーションを行える。
仮想マシンのみ移動：vMotion
ストレージのみ移動：Storage vMotion

・仕組み
補足資料P50
仮想マシンのファイルは共有ストレージ上のため、ファイルの場所を動かすわけではない。
メモリの引継ぎ

vMotion開始
移動先ESXに逐次メモリコピー
一時停止
残りのメモリコピー
移行先で仮想マシンプロセス起動
⇒仕組み上一瞬は止まるので、アプリが許容するかどうかは検証が必要。(RTTも10-30msec程度伸びる)


・vMotion用のVMkernelポートを作成する。
・vMotion用NWと本番NWは分ける。帯域的にはNICレベルで分けた方がいい。


・vMotion仮想マシン要件
⇒vMotion前後で同じであること。
移行先ESXが同セグのNWを持つこと。(ゲストのIPへの到達性)
# vMotionのメモリコピーNWはL3跨いでよい
ホストのローカルイメージがマウントされた仮想デバイス(CDなど)をマウントしていないこと。
RDM利用お場合は移行先ホストがRDMファイル/LUNにアクセス可能であること

・vMotionホスト要件
仮想マシンは共有ストレージ上であること
VMkernelポートでvMotion有効化済み
移行元/先
1Gbps以上のNW
 最低250Mbit/sec
 1G 同時最大4件,10G 最大8件
パフォーマンス改善には2つの専用ポートグループが推奨

★移行元/先でホスト CPU機能セットに互換性が必要
  ⇒CPUメーカー/Family/世代 の一致が必要

  制限緩和設定として
  Enhanced vMotion Compatibility(EVC)か互換性マスクを使い一部の機能を認識させない

  Intel <-> AMDはどうやっても無理。

vMotion実行時に要件を満たしているか検出がされる。

・vMotionの暗号化(vsphere 6.5～)
6.5から仮想マシンの暗号化ができるようになった。
VM暗号化ありの場合はvMotion暗号化必須
暗号化vMotionにはKMS(鍵管理)サーバや証明書など必要。

Disabled

Opportunistic 非暗号化VMのデフォルト。
⇒移行元/先 ホスト両方が対応していればやる。

Required 暗号化VMのデフォルト
⇒移行元/先 ホストが暗号化vMotion負荷の場合仮想マシンを移行しない。


・vCenter間の移行
リンクされたvCenter間でvMotion可能となった
MAC被りがないかをチェックしてくれる。

・VMkernelレベルのTCP/IPスタック
昔(～5.X)はホスト全体で単一ルーティングテーブルであり
管理用VMkernelポートにしかDGWが設定できなかった。
vMotionやStorageがL3通信するにはstaic route必須だった
6.x～からVMkernelごとに独立した。
デフォルトは同じものを利用、vMotionだけ独立させるなどが出来る。

・Long Distance vSphere vMotion
正確には距離でなくRTTの制限緩和。
10msec以下である必要があったが(5.x)150msecまでになった(6.x)
国内ならダイジョブ
# コントロール不能なキャリアの品質上。

◆Enhcaned vMotion Coompatibility(EVC)
デフォルトは無効。CPUの互換性がvMotionに必要
クロック速度/キャッシュサイズ/HT,Core数：一致不要
メーカー/ファミリー/世代：一致必要
拡張命令セットのバージョン：一致必要
仮想化のハードウェアアシスト：一致必要(64bitゲストOSのみ IntelVT)

上記要件を緩和するクラスタ機能。
個々のESXでなくクラスターに対して設定する。
新しいCPUでしか使えない機能を使わないことで新しい世代のCPUに下位互換を持たせる。
よって、クラスター内で最も古いCPU世代をベースラインにする。

・EVCクラスタの要件
単一ベンダーのCPUであること(intel/AMD混合不可)
intelはMromマイクロアーキテクチャ移行
AMDは第1世代Opteronより新しいCPU

ハードウェア仮想化対応であること(Intel VT/AMD-V)
実行無効化テクノロジー(セキュリティ機能))に対応であること(Intel eXecute Disable(XD)/AMD No eXecute(NX))

・EVCモードの変更
古いCPUがクラスタからなくなったのでベースラインを引き上げたいなど

引き上げ
⇒VMはパワーオン状態で可能。ただし、EVCの新機能(新しい機能)は次回VM起動時に反映される。
  VM起動時にCPUの世代/命令セットを調べるため

引き下げ
⇒VM止めないとダメ

・仮想マシンEVCモード(6.7～)
仮想マシン単位でEVCモードを適用できる。クラスター外のVMにもEVC適用可能となる。
EVCはVM単体/クラスター単位/併用が可能。併用時、EVCモードが異なる場合はVM単位が優先。

・仮想GPUのEVC(7.0 U1～)
vGPUでもEVCが可能となった。クラスタ内のGPU機能セットベースラインを定義する。


◆Stroage vMotion
電源OnのままVMのファイルを移動できる。
vsphere5.5～ではI/Oが瞬断したりはしない。
移行中の書き込み処理はミラードライバーという機能（サービス）により移行前、
移行中の仮想ディスク両方に同じ書き込みが行われるため停止が起こらない仕組み。


・使い道
データストアのリプレース、メンテ
ストレージの移行
データストア間の容量を均す


実行時にプロビジョニングモードの変更ができる(Thin -> Thickなど)
仮想マシンファイル名は再セットされる

・動作
- ESXの機能でやる
  データの移行をESXの機能を利用(Data Mover)
  長時間負荷がかかる

- ストレージの機能でやる
  ストレージがStorage vMotionに対応している場合(API: Array Integration)、Storage側の機能を利用できる。
  vSphereがArray Integrationに対応しているか自動検出し、対応していればストレージに処理をオフロード、そうでなければData Moverを使う。
  # 設定のストレージデバイス ハードウェアアクセラレーションで確認可能。

・制限
独立型仮想マシンディスクは パーシステントモードになっている必要有

仮想ディスク モード
・通常型(デフォルト)
⇒スナップショット実行可能。独立型は不可。

.独立型(パーシステント)
⇒スナップショット実行負荷

.独立型(ノンパーシステント)：Storage vMotion負荷
⇒スナップショット実行負荷
  VMに変更をしても仮想マシンの電源を切ると変更したデータは消える。一種の読み取り専用VM

・Shared-Nothing vMotion
vMotionとStorage vMotionを同時に行うもの。
共有ディスク不要でvMotionできるようになる。
# 機能的には上記２つの機能を同時実行だが、ドキュメントでは別の機能として扱われている


◆スナップショット
スナップショットを取った状態まで巻き戻しできる機能。
変更前にスナップショットを取得し、問題があればスナップショットから戻すなど。(最大32個)
一時的に巻き戻可能にする利用用途でありしバックアップではない。
スナップショットの保持は3日以内が望ましいとされる。それ以上が必要ならクローンなど別の手段検討

仮想マシンの構成状態(仮想HWの設定/メモリ状態(オプション)/仮想ディスク)を取ってくれる
独立型仮想ディスク((ノン)パーシステント)は使用不可

・メモリの状態も含める
snapshot対象にすれば、戻してもpoweronの状態を維持する。
その分snapshot作成に時間はかかる。
戻しの時の負荷が上がるので、その時のホストリソース次第ではハングアップしたような状態になることがある。

スナップショット取得後は元のディスクは読み取り専用となりその時点の状態を保持する(.vmdk/-flat.vmdk)
新たな書き込みはスナップショット用の差分ディスクに書かれれる。
# 最も大きい差分サイズは元もディスクと同容量

差分ディスクの形式は自動的に決まる
VMFSsparse
⇒古めの形式。VMFS5かつ、VMのディスクが2TBより小さいこと。-delta.vmdkファイルとなる。

SEsparse
⇒VMFS6 or VMFS5+2Tより大きい仮想ディスク（NFSでもこちら) # -sespase.vmdk
  VMFSスパースと違いディスクの再利用が可能なので、差分が小さくなった場合はスナップショットファイルも小さくなる。

vsanSparse
⇒vSAN用。# 差分オブジェクト


◆スナップショットファイル
-Snapshot.vmsn # テキスト 設定の状態
-Snapshot.vmem # メモリの状態(オプション)
-00000x.vmdk #   ディスクディスクリプタ
-00000x-delta.vmdk # VMFS5差分
-00000x-sesparse.vmdk # VMFS6差分
.vmsd # 全仮想マシンスナップショットの名前、説明、関係


◆スナップショット削除パターン
スナップショットのマージはディスク負荷がかかる。
バックアップに利用していけない理由の１つ(バックアップは長期なので差分データが大)

base -> snap1 -> snamp2 -> here

・snap1削除
snap1データがbaseにコミットされる

・snap2削除
snap2データがsnap1にコミットされ、snap2の-delta.vmdkは消える。
⇒これは危険パターン。
  snap1にsnap2のデータがコミットされる ⇒ 本来のsnap1に戻せなくなる。
  ⇒snap1に巻き戻して、現在点をsnap2より前にしてからsnap2を消すのが正しい
    base -> snap1 -> [here] -> snamp2 
    この状態にしてから消す。

・全削除
snap1 ベースコミット後、snap2ベースコミットされる。

・スナップショットの統合
何らかのエラーによりスナップショットマネージャにスナップショットは表示されるが
データストアに差分ディスクファイルが残っている場合、一連差分をベースディスクにコミットする方法。
# 警告に統合が必要である旨が出る(Virtual machine Consolidation Needed status)

スナップショットメニューの統合(Consolidate)から再度削除処理ができる。

◆vSphere Replicationとバックアップ
・vSphere Replication
仮想マシンバックアップ機能(vCenterの拡張機能。追加のライセンスは不要。※エッセンシャルは除く)
SRMみたいな行動な機能はないが最低限のDRは可能。導入/設計が楽
DR自体はストレージの機能で実装(サイト間レプリケーション)で実施していることも多い。

vSphere ReplicationアプライアンスOVFをDLして展開して使う。

・レプリケーション機能
ソース/ターゲットサイトのVMレプリケーション
変更ブロックをレプリケーションする。
最初はフルコピーとなるので大変だが、2回目以降は差分のみとなる。

VRアプライアンスの展開とESXiへのVRエージェントの導入が必要。

◆リソース管理

・メモリのオーバーコミット
実際にホストのメモリが不足すると、VMのパフォーマンスに大きく影響を与える。
仮想マシンは起動時に仮想メモリと同等サイズの.vswpファイルを作るので(電源Off時に消す)
メモリが足りなければこちらにスワップする。
ESXはなるべくスワップしないように努力してくれる。

1.透過的なページ共有
同じメモリページを検知して重複排除する。
デフォルトでは仮想マシン内で有効で仮想マシン間の重複排除は無効。
共有するVMの組み合わせを選ぶこともできる。(GUIでの設定ではない。仮想マシンのAdvance setting)

ベストプラクティスは仮想マシン内での共有のみ。
仮想マシン間共有がよくないとされる理由はセキュリティリスク。

# そこまで大きな効果が出る機能ではない。

2.バルーニング
VMware Toolsが必要。入れると自動的に有効になる。
VMware toolsのバルーンドライバで仮想マシンのメモリを開放する。
ESXはは他のVMにメモリ割り当てることができるようになる。

メモリはVM電源On時に割り当てられ、電源Off時に開放されるが
バルーニングをすると起動中のままメモリが開放できる。
メモリが不足しているときに動作し、仮想マシンには自分のページング領域を使わせる。

3.メモリの圧縮


4.ホストレベルのSSDスワップ/仮想マシンメモリのディスクへのページング
最終手段。VMkernelスワップ領域(vswpファイル)を使う。
体感で明らかにわかるレベルで遅くなる。
SSDにすればちょっとマシになるので、vswpファイルだけをSSDに置くなどはできる。


◆メモリの最適化
余計なトラブル回避のため、メモリはオーバーコミットしないのが妥当。(少なくとも本番は)
CPUと違いオーバーコミットなしが容易。
オーバーコミットしなければ上記機能は不要となる。


◆CPU
オーバーコミットになることが多い。

# 備考
CPUスケジューラがVMに実際のCPU(コア)を動的にアサインしロードバランスする。
仮想化環境ではCPUを増やすと逆に遅くなることがあり、制御が難しい。

VMに割り当てられるvCPUは1ソケット内に

・ハイパースレッド
CPU/BIOS/ESXiでHTが有効であれば使える(ESXはデフォルト有効)

・サーバが(v)NUMA対応の場合はメモリアクセスを最適化できる。
 物理CPU跨ぎでvCPU割り当てはできるのだが、効率が悪く遅くなるのでNUMAがある。  
 同じVMは同じCPUで割り当てる形にする。

◆予約/制限/シェア
・予約
VM割り当てリソースの最低保証値
CPU/Mem:デフォルト 0

 RAM
 vswpサイズ = 制限値 - 予約値
 予約領域の確保はVM起動時に行われる。
 起動時に予約領域が確保できない場合、パワーオンできない。
 予約領域はバルーンの対象外となる。

 CPU
 起動時に確保されるが、使っていないときは解放される(必要な時は使える)

・制限
VMが利用可能な上限。
通常CPU制限を指定する必要はない。
CPUが開いているのに使えない状態にする意味はないし
開いていなければスケジュールの優先度で制御すればよい。(シェア)

CPU:デフォルト無制限
Mem:VMの割り当てメモリ

・シェア
仮想マシン間の相対的な優先値。
シェアの対象は予約領域は含まれない。予約されなかったCPU領域などがシェアの対象になる。

VM-A 1000
VM-B 2000
の場合、VM-Aは1/3、VM-Bは2/3の割合になる。

シェアの計算対象は稼働中のVMに対して行われる。(パワーオフは対象外)
仮想マシンの増減で、同じシェア値でも割り当てられる割合は変わる。

デフォルト値は設定値で変わる。設定のデフォルト値はNormal
CPU
CPUあたり * 設定値(Normal 1000シェア)

Mem
1MB当たり * 設定値(Normal 10シェア)


任意の整数を入力するカスタムで設定する場合微調整をできるようにするため
4桁程度で設定するのがよいとされる
1000:1000と1:1は同じだが、より細かいシェアを設定できるのは1000:1000のため


・esxtop
ESXで実行。resxtopは廃止。# vCenterはvimtop
ブラウザと異なり、リアルタイムにパフォーマンスが見れる。(2～5秒間隔)
top同様キーボードで表示を切り替える。(c -> V 仮想マシンのCPUメトリック)

・パフォーマンスチャート
リアルタイムのesxtop/長期傾向分析のGUI併用して使うのがよい
# リアルタイム(過去1時間)のデータ収集間隔は20秒
クラスタ/ホスト/データストア/ネットワーク/仮想マシン などが見える。

Overview
基本的な項目一覧。カスタムなどはほとんどできない

Advanced
細かい項目やカスタムしたチャートが見れる

・チャートの期間/履歴
過去1時間/1日/先週/先月/過去1年とあるが
データ収集間隔はどんどん荒くなる。1年前のx時y分のデータは見れない

ESXホスト選択時のみ見れるチャートもある(仮想マシンごとの積み上げグラフ)

・ロールアップ
統計期間の変換機能。
過去1hにおける5分間の情報を過去1日の値に変換するなど。DBの値はロールアップされたもの。


・CPUの確認観点
利用率

待機時間率(CPU Ready)
CPUオーバーコミット時のCPU空き順番待ちがどのくらいか。
shareの値が相対的に低いと順番待ちが長くなるので大きくなりやすい

相互停止
マルチコアの場合に見る必要がある。
スキュー：複数vCPUで片方は確保できたが、もう片方はオーバーコミットの場合
          vCPU2はvCPU2に対し処理が遅れる状態。

スキューを解消するための動作。確保できたCPU同期を補正して足並みをそろえる。結果遅くなる
相互停止が高いVMはvCPUを減らすとスキューの相互停止がなくなるため逆に早くなる。

・メモリの確認観点
ホストレベルでswapファイルを使っているかどうか。
# 兆候までみたければバルーニングも見る


・ストレージの確認観点
遅延がどこで発生しているかがポイント(難しい)

Read rate/Write rate 
⇒IOPSのこと

Read latency/Write latency
⇒遅延が大きければIOPSも落ちるので相互関連する。


ディスク遅延
・kernel command latency
こっちが大きいのはホストの内部プロセスが高負荷の傾向

各SCSIコマンド処理にVMkernelが要した平均時間
高い(2-3msec以上)はアレイ負荷 or ホスト負荷が大きい

・physical device command latency
こっちが大きいのはホスト外がネックの傾向(ネットワーク/ストレージ)
外のどこが悪いか特定するのは難しい

物理デバイスがSCSIコマンド完了に要した平均時間(esx -> iscsi -> esxの行きかえり)
HDD 15-20msec以上
SSD 3-4msec以上が遅い

◆アラーム
vCenterで管理しているオブジェクトでのイベントや変化をユーザに通知する
デフォルトで沢山アラームを持っている。
新しくアラームを作る場合はデフォルトをコピーして作成できる。

イベントか状態を監視するかの2種類ある

・アラームリセット
ルールで定義できる。
ex)メール/SNMP通知したらアラームはリセットする

作成したルールはデフォルトで有効化状態となる。
適用単位は各オブジェクトを選べる(vCenter/DC/フォルダやクラスタ/VM)


◆クラスタ
1クラスタに96ESXがおける(7.0U1～)

・クラスタ機能
DRS/HA/vSAN

全ホストのイメージセットアップ/アップデートを纏めて行うこともできる。
※ただし1回設定すると二度と無効にできない

手動もしくはクイックスタートを使って作ることができる。
※クイックスタートは必須パラメータが一連の流れで登場するので、設定漏れがないのが利点


◆vSphere DRS
クラスタ内でパフォーマンスが得られていないゲスト(仮想マシンの配置)を改善するため
自動vMotionする。

# Storage DRSという機能もあり、配置Storageを自動的に変更する

6.7/7.0でDRS基準が違う
6.7はホスト視点
ホストのCPU/Memの使用率を見て、ホストのCPU/Memの使用率のバラつきを是正したら結果として仮想マシンのバラつきもなくなる

7.0はVM視点
他のホストに移動した方がVMがよりパフォーマンスを発揮できるか？と考える
仮想マシンにスコアを付けて、スコアが低いVMは今このホストにいるべきではないと判断する。
DSRスコアは0～100%の間で付けられる。0%が最もリソース競合している状態
クラスタの監視 - DRSスコアやパフォーマンスチャートで一覧が見れる

スコアの判断は1分に1回される(7.0/6.7は5分に1回)


DRSはクラスタ設定でボタン一つで有効化できる
DRSの設定値はデフォルトが推奨値

・Automation Level
クラスタ全体に設定するが、VM単位でオーバーライドすることができる(DRS Disableも可)
          VMの初期配置   ロードバランス
------------------------------
手動        手動配置     手動管理(通知のみ)
部分自動化  自動配置     手動管理(通知のみ)
完全自動化  自動配置     自動化

VMの初期配置: VMの起動時に好スコアが出そうなESXで起動する
ロードバランス：クラスタ内の負荷の偏りをユーザ通知のみか、vMotionまで自動でやるか。

Priority
DSRの移動推奨の度合い。1～5。1が最高。5はやっても大差ないらしい。
手動管理の通知に付帯する情報。

・Migration Threshold
PriorityがいくつならvMotionさせるか。デフォルトは真ん中(1-3をDRS実行)
手動管理の場合はユーザへの通知

・Predictive DRS
DRSは悪くなったものを改善するが、Predictive DRSはこれから悪くなりそうなものを予防する。
リソース情報からこれからスコア悪くなりそうなもの予測しDRSする機能。
vRealize Operations Mangerが必要(別ライセンス)

# 備考
・Proactive HA
ハードウェア(SIMプロバイダ)とESXで連携し、ハード障害の警告に基づきDRSで非難させる
サーバのHWが対応している必要がある。


◆アフィニティ/非アフィニティ ルール
DRSにより、Actvie/Standbyの関係があるVMが同一のESXに配置される可能性がある。
また、同一時間帯に高負荷になるVMが同じESXで動くのは望ましくない。
それを避けるために同一のESXで動かさない設定をすることができる。

・非アフィニティルール
ルールに紐づいた仮想マシンを選択した仮想マシンを異なるホストに配置する

・アフィニティルール
選択した仮想マシンを同じホストに配置する
ゲスト間で大量の通信を発生させるものを同一ホストに置くことで
物理NWにトラフィックを流さずに済む。

どちらも、どのホストに置くかまでは設定できない。


VM GroupとHost Groupを作成し関連づけ、ルールを設定する。
◆アフィニティルール
・優先ルール
必要に応じて違反しても構わない。
たとえば、障害で指定したホストグループにVMを置けない場合は、別のホストグループにおける。

・必須ルール
違反は許さない
障害が発生してもほかのホストグループで復帰させることができない。
ホストベースのライセンス適用など。

◆DRSクラスタの要件
・vMotionが可能であること
・shared-nothing vMotion不可(共有ディスク必須)

◆メンテナンスモード
メンテモードのESXはVMを動かせなくなる。
メンテ時に、DRSでメンテ対象のホストにVMがvMotionされてくることを防いだり
まだVM稼働なのにESXを誤ってshutdownすることを防げる。

また、メンテモードにすることで、ホストをクラスタメンバーから外すことができる。
クラスタリソースからメンテナンスモードホストのリソースが削除され利用できなくなる。
⇒残りのホストのCPU/MemだけでVMを稼働させる必要がある。

メンテナンスモードにする際は、実行中のVMは別のホストに移行するか、shutdown/サスペンド状態にする必要有
DRSが完全自動化モードの場合、パワーオンのVMは自動的にvMotionされる

◆スタンバイモード(DRS機能)
vSphere DPM(消費電力削減機能)でホストがスタンバイモードに切り替わると
DRSで仮想マシンをどかしてホストはパワーオフされる。
⇒余剰なホストを電源Offにして電力節約する。
リソースが必要になったら電源を入れてくれる。
⇒ネットワーク経由で起動するため、ハードウェアサポートが必要。(iLo/IPMI/WoLなど)

・DRSと動的 DirectPath I/O (vSphere7.0～)
従来Direct Path I/OはDRS不可だったができるようにした。
同じHW構造を持つサーバを定義すれば、vMotionの対象にできる。
ただし、DRSは起動時のみでロードバランスはしない。

◆vSphere HA
主にESXiホスト障害に対処する。ESXホスト障害時ほかのホストでVMを再起動する
他にも仮想マシン障害/アプリケーション障害/データストアアクセス障害/ネットワーク分離などの対応機能もある。

設定難易度が高め。使いたい機能を有効化し、ユーザのニーズに合わせて細かく設定する必要がある。
要件次第のため殆どの設定はオプション。DRSのようにデフォルトが最適ではない。

仮想マシンの停止は避けられない。復旧時間はアプリの起動時間次第。
短時間で復旧できるのが利点。

VMは共有ストレージである必要がある。
HWの要件は低く、CPUの互換性は不要。(いったん止まっているので)

・ホスト障害
デフォルト有効
別のESXで再起動する

・ゲストOSの障害
デフォルト無効
VMware Toolsを利用。仮想マシンがハートビートの送信停止や仮想マシンプロセス(vmx)がクラッシュした場合に仮想マシンをリセット。
アプリケーションのダウンに反応はできない

・アプリケーションの障害
デフォルト無効
アプリケーションハートビートを利用し、NG時はVMごと再起動する。
再起動により復旧できるものに対して有効。

・データストアへのアクセス障害
VMCP(仮想マシンコンポーネント保護)有効時の動作を設定する。

・ネットワークへの接続障害
あまり使わない



ESXi間でハートビート(UDP)パケットを送りあう。
ハートビートは原則管理用NW(VMkernelポート)を利用して送受信する。
⇒管理NWの断に対する考慮が必要。(スプリットブレイン)
  ⇒冗長ハートビートネットワークが重要

・vSphere HAクラスタのハートビート特性
1クラスタに対してマスターが1台。それ以外はスレーブ
マスターはすべてのスレーブを監視する
全スレーブはマスターを監視する

・冗長
管理用VMkernelポートのアップリンクポートはNICチーミングする。
# 例外として、vSANの場合はvSAN用のVMkernel経由でハートビートが送られる。

管理用のVMkernelポートを2つ(2IP)作成して別々のアップリンクポートに繋ぐのも可

★障害見地については追加調査が必要そうだ。

◆HAのアーキテクチャ
HA有効時、ホストでFDM(Fault Domain Manager)サービスが起動される
# HAのエージェント

FDMはマスターの選定を行う。
マスターFDMはvCenterにクラスターステータスを通知(状況の通知)

スプリットブレインを考慮し、2段階のハートビートを持つ
・ネットワークハートビート
管理NWを使い、Master <-> Slaveでハートビートを送信する。

データストアハートビート
ホスト - データストア間のハートビート
データストアのファイルをロックすることで他のホストに自ホストが稼働していることを通知する。

NWハートビートNG/DCハートビートOK
⇒ネットワーク障害と判定
  ⇒スレーブ側が切れたか、マスター側が切れたかの判定が必要。
    ⇒どこが切れたらどうするかを個別に設定する。これが難しい。

NWハートビート/DCハートビート両方NG
⇒ホスト障害と判定


・スレーブ障害時
マスターがどのホストでVM再起動するかを決める

・マスター障害時
スレーブのどれかがマスターに昇格する。
マスターに昇格後、マスターはVMを再起動する
マスター選定プロセスは15秒程度かかるので、すこし遅くなる

・マスターの選定基準
1.最も多くのデータストア数を持つホスト
2.HA有効化時に各ホストに自動割り当てされる管理オブジェクトID(MOID:モイド)が大きいホストをマスターとする。
  ※MOIDの大きい値とは
  左から1桁ずつ比較する。98,99,100であれば、まず一番左の位である9,9,1を比較しついに8,9(,0)となる
  よって大きい順は99,98,100となる。 

・HAの自動フェイルバック
ない。戻したければvMotionする

※HA実行はvCenterがダウンしていても問題ない。HAの動作はFDMが実行する。

・隔離されたホスト
基本HAはネットワーク障害は対処不要と考える。(ハートビートは管理NWのため)

# option
管理用VMkernelポートからDGWへのpingと併用して
管理NWとサービス系を一緒にしている構成の場合に影響範囲を判断することができる

DGWのことを隔離アドレスと表現する。


・ストレージ障害(6.0～)
ストレージパスを失ったESXのVMを別のESXにて再起動する。
仮想マシンコンポーネント保護(VMCP)を有効にするとデータストアへのアクセス障害を検知できる。

PDL(Parmanent Device Loss:永続的ダウン) 復旧は長期になるとvSphereが想定したもの
APD(ALL Path Down) 短期での復旧を想定

⇒ストレージに対してSCSIコマンドを発行してどう帰ってきたかでどちらか判定する

・HAの設計検討事項
冗長ハートビートNW/冗長隔離アドレス(DGW)
データストアハートビートのNWは管理NWから物理的に分ける


★設定
・仮想マシンの監視
クラスタ全体で定義し、個別要件は仮想マシン単位でオーバーライドする。
ハートビートデータストア
⇒どこを使うかは手動選択すべき。管理NWでないものを選ぶ事が極めて強く推奨されるため。
  自動は共有ストレージで最も多くのホストが使われているものを選ぶ。

・アドミッションコントロール（入場制限）
最重要設定
補足資料P72

ESX1が落ちた場合、ESX2に全ゲストが稼働できるリソースがあるとは限らない。
リソースが不足していた場合、パワーオン失敗する。特に予約リソースを持つVM。
仮想マシンのFailover用にクラスターからリソースを事前確保する。

フェイルオーバーキャパシティの定義

- スロットポリシー
スロット：仮想マシン1台を起動するのに必要なリソースサイズの概念
各ホストで何台のVMが稼働できるかを事前に自動計算し、スロット数を算出する
算出は手動もできるが自動が推奨される

4スロットのESXが3台の場合は合計12スロット。
1台のESX障害まで耐えるには、4スロットの予備が必要で、3台合計8スロットがパワーオンできる。(超えた分はパワーオン不可)

# オーバーコミットと相性悪くない?
# ⇒あくまで予約リソースベースで算出するので直接関係ない。

- スロットサイズの算出
起動状態のVMから自動算出
CPU/Memスロットサイズ
 予約の中で最も大きいもの。
 予約がない場合は
 CPU:32Mhz
 Mem: クラスター内で仮想化メモリオーバーヘッドが最も大きいもの。

CPUスロットサイズ 1Ghz/Mem 2GBとして
ホストが3GHz 2core/Mem 8Gの場合
CPU:6台/Mem:4台なので低い方に合わせて4台と考える。

ベースのスロットサイズは大きい方に合わせるので、予約が巨大な仮想マシンや
予約の程度が不均衡だった場合起動できるVMが制限されるのが欠点。
全部が同程度の予約であれば有効な考え方。


- クラスタリソースの割合(%)
主流の設定。クラスタ総リソースのn%を予備とする。

- 専用フェイルオーバーホスト
障害があった場合にのみ利用するホストを用意し、指定する。



・仮想マシンで許容されるパフォーマンスの低下(6.5～)
DRS+HAで使う(意味がある)設定。
HAの実行した結果、もともといたVMのパフォーマンスが低下したことを警告する。
どの程度低下したら警告するかを設定する。0～100%で設定し、100%は警告しない。
DRSが併用されている環境でなければ機能しない。

・Default VM Restart Priority
どのVMから順番に上げるか。
デフォルトは全VMが同じ優先度

クラスター設定をVMでオーバーライドして使う。

例外)
エージェント仮想マシンは最初に起動される

・(参考)メモリ/CPU スロットサイズの最大値を設定
予約の最大値を基準にすると無駄が大きいので、最大値を設定する。
(特に1台だけ突出した予約がある場合)

・再起動のオーケストレーション
VM起動の優先度をつけられる
ただ、電源を先に入れる=アプリが先に起動完了するではない。
そのため、再起動準備が整ったと判断する基準が設定できる。
VMware Toolsアプリケーションのハートビートを検知して次に行く、などが出来てアプリの起動順序の制御ができる。
ただし、最後のVMが起動するまでの時間は相応に伸びる。

依存関係の設定(6.5～)
優先度が同等以上のVMは依存関係を設定できる。


・Enable Host Monitoring
ネットワークハートビートの有効無効。
ネットワークのメンテナンス時、ハートビートが切れる時のみ外す。

・HAとDRSの併用
HAは切り戻しができないが、DRSはスコアに基づいて自動切り戻ししてくれる。

HAは以下の理由でFoできないことがある
アドミッションコントロール無効/不適切で残りのホストに全VMを起動するリソースがない
総リソースは足りているが、ホストで分断されている。HAはDRSががれば、vMotionしてクラスタを調整する


◆vSphere FT
予め同一VMを2つ実行/同期しておき障害時に瞬時に切り替える。OSの依存などはなし
HAと異なりダウンタイムなし。より重要度の高いVM保護。
ホスト障害時切り替える機能で、VM障害時はFT動作しない。
よって、自動的に別ホストにプライマリ/セカンダリが配置される

同期をとりづけるため、セカンダリマシンはコンソール操作などは一切できない。

・制限
8CPU 128GMem
4FT VM/1ホスト
CPU互換性必要(裏でvMotion)
EVC有効の場合DRS可能。
vShprer HA設定必須。(HAクラスター内のVMにFTの設定をする)

通常の2倍リソース必要。
FTのVMはメモリの設定値が100%予約される

切り替え時
プライマリ/セカンダリで構成。プライマリが落ちたらセカンダリがプライマリに昇格。
新セカンダリが別ホストで実行される。

プロセス/仮想マシンファイルは全く別々のものとして作成される。

・FT用ネットワーク
10Gbpsのネットワークが必要。

プライマリVMの変更はセカンダリVMで処理されず、
FTネットワークを経由してセカンダリのメモリがアップデートされる。
また、Pri/Secの死活監視もここで行われる。

FTの同期(チェックポイント)
定期的でなく、動的にチェックポイント間隔が調整される(数msec～数100msec)

FTネットワーク切断時
データストアを使う点はHAと同じ。
FTはPri/Secでファイルが違うので、.ft-generationファイルでスプリットブレインを監視する。
# ファイル名を変更(末尾に数値付与)で通知

shared.vmftファイル
vSphere上の全てのオブジェクトは一意なUUIDを持つ。（なお仮想マシンのUUIDは.vmxの中にある。）
shared.vmftファイルにはプライマリのUUIDを持つので、SecがフェイルオーバーしてPriになったときに
PriのUUIDに変更でき、PriのUUIDを一貫させられる。


◆vSphereクラスタ サービス(7.0 U1～)
vCenter障害時にクラスタ機能を失わないようにする。(HAは使えるが、DRSは使えなくなる)
⇒vCenter障害時にDRSを使えるようにする機能。

7.0U1では意識しなくとも勝手に有効化される。
クラスターを有効化するとクラスタにvSphereクラスタにクラスタサービス仮想マシン(vCLS VM)を展開する。

DRS使ってなくても勝手にvCLSが勝手に生えるので、その場合は削除する。
(vCenterにvCLSのOVFがバンドルされている)

vCLS VMはいるだけでOKなのでNIC/IPなどは不要。(割り当てられない)
vCLS VMはインベントリからは見えない。管理 - vCenter Server Extendtions - Agent
vCLS VMのOVFで新しいアップデートがあった場合、自動的にクラスタに行われる。
削除のやり方は少し特殊なのでドキュメント参照。


ライフサイクルマネージャ
◆vCenter Server Upate Planner
vCenterに登録されているVMawre製品のアップデート情報の表示
互換条件のチェックもできる(vCenterをupdateすると関連の～はアップデート何にするか？など)
CSVエクスポートも可能。
⇒あくまでアップデート関連情報の提示

インストールなど不要
ライセンスEnt+でないと使えない

vCenterをインターネットに接続しておくと、自動的にupdateをチェックしてくれる。

◆Lifecycle Manager
アップデートマネージャの後継。

ESXiだけでなく、ハードウェア自体のBIOS/ファームウェアまでも管理することができる。
install不要。すぐに使える

ESXiホストのアップデート/パッチ適用
ESXiホストのサードパーティーソフトのinstall/update
ハードウェアへのファームのインストール ※ハードウェアが互換するvSphere用のドライバを持っている必要がある。(HPE/DELL)
ESXiドライバ/ファームのinstall/update
VMwareToolsのinstall/update

インターネットにつながっていればパッチのダウンロードを自動的にしてくれる
(update managerは情報だけ。パッチはこっち)

・アップデート管理は2つの方法から1つを選ぶ
ベースラインを使用した管理(古い UpdateManagerベース)
ISOとVIB(ビブ)でESXをアップデート

イメージを使用した管理
 クラスター単位で管理。
 クラスター内のホストがすべて同じメーカであることが前提(HPE/DELL混合不可）理想は型番も同じ。

ベースライン ⇒ イメージの変更は可能だが、逆は不可。
いったんイメージにしたら戻せない。

・DRSの統合
DRSを使っていると自動的にLifecycle Managerと統合される。
ローリングアップデートを自動的にできるようになる。(DRS 完全自動化の時)
パッチの導入時にメンテモードにすると、DRSがゲストを退避させてくれる。
全ESXにパッチを充てるまで順番に行われる。


◆ベースラインの利用
古いESXがある場合やupdate managerに慣れている場合

ベースライン:複数のvibの集合。これをホストにインストールする。

固定パッチベースライン：
動的パッチベースライン：重要度が高いパッチを自動的にベースライン作ってくれる。セキュリティ系の修正などに。

市販のソフトを入れている場合はそのvibを含むベースラインを作ることになるがコマンドで作ることが多い。

事前チェック
これから入れるパッチがインストール先に適合するか事前にchkできる

インストール
ESXはメンテモードで実行する必要がある。

◆イメージの使用
ESXiのイメージを作成し、それをインストールする。
警告はでるが、イメージに単体のvibを含めることはできる。

全ESXiがv7以上
全ESXiがステートフルモードでインストールされている
  ステートフル
  ステートレス
  ステートフルキャッシュ (ネットワーク経由)
の3つある 

イメージは以下の要素で構成される
ESXi基本イメージ
コンポーネント
ベンダーアドオン
ファームウェアとドライバのアドオン

イメージのintallはクラスター単位で行われるので機種レベルでESXを統一すると
ファームウェアとドライバのアドオンの適用(FWのアップ)が楽。

・VMware Toolのアップデート
Linuxのopen-vm-toolsはGuest Managedとなり、ライフサイクルマネージャでは管理不可。個別に実施する。
あくまで公式のtool配布。

Toolのアップデート前に自動的にsnapshotを取ってくれ、終わったら時間がたったら自動的に消してくれる

・仮想マシンハードウェアのアップデート
これも管理官可能。













